{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de dados\n",
    "- esse Jupyter Notebook mostra como é feita a limpeza de dados"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importações"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regex (expressão regular)\n",
    "!pip install unidecode\n",
    "from unicodedata import normalize # utíl para tratamento de texto e compatibilidade\n",
    "!pip install nltk\n",
    "import nltk # Ferramentas de PLN\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt # Plotagem de gráficos\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Importações feitas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"No meio do caminho tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\n",
    "Tinha uma pedra\n",
    "No meio do caminho tinha uma pedra\n",
    "\n",
    "Nunca me esquecerei desse acontecimento\n",
    "Na vida de minhas retinas tão fatigadas\n",
    "Nunca me esquecerei que no meio do caminho\n",
    "Tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\n",
    "No meio do caminho tinha uma pedra\"\"\"\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Meu texto:\" + \"\\033[0m\")\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixando tudo em minúsculo\n",
    "texto = texto.lower()\n",
    "# Removendo a pontuação (EX: \"!?.,/|#$%¨&\")\n",
    "texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Meu texto em minúsculo e sem pontuação:\" + \"\\033[0m\")\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0]+\"\n",
    "# Bag-of-words: Conjunto não-ordenado de palavras e sua multiplicidade\n",
    "# Cria um \"Bag of Words (Saco de Palavras)\" do nosso texto e ignora números\n",
    "sacoDePalavras = re.findall(regex, texto)\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Meu Saco de Palavras:\" + \"\\033[0m\")\n",
    "print(sacoDePalavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma stopword pode ser considerada uma palavra irrelevante para a análise\n",
    "nltk.download('stopwords')\n",
    "# RSLP(Removedor de Sufixos da Língua Portuguesa)\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um vetor de stopWords\n",
    "stopWords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Minhas Stop Words:\" + \"\\033[0m\")\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo StopWord do nosso texto\n",
    "for palavra in stopWords:\n",
    "    while palavra in sacoDePalavras: \n",
    "        sacoDePalavras.remove(palavra)\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Meu Saco de Palavras sem Stop Words:\" + \"\\033[0m\")\n",
    "print(sacoDePalavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical)\n",
    "stemmer = nltk.stem.RSLPStemmer() # RSLP(Removedor de Sufixos da Língua Portuguesa)\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Meu Saco de Palavras sem Stop Words:\" + \"\\033[0m\")\n",
    "print(sacoDePalavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = nltk.stem.RSLPStemmer() # RSLP(Removedor de Sufixos da Língua Portuguesa)\n",
    "\n",
    "wordCount = dict()\n",
    "for palavra in sacoDePalavras:\n",
    "    # Removendo acentuação\n",
    "    palavra = normalize('NFKD', palavra).encode('ASCII','ignore').decode('ASCII')\n",
    "    # O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical)\n",
    "    palavra = stemmer.stem(palavra)\n",
    "    wordCount[palavra] = wordCount.get(palavra, 0) + 1\n",
    "\n",
    "print(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + \"\\033[1m\" + \"\\033[92m\" + 'TESTE DE PLOT')\n",
    "plt.xticks(rotation='60')\n",
    "plt.bar(list(wordCount.keys()), wordCount.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polissemia (muitos significados) é um problema\n",
    "# Sinônimos também podem dificultar a analise\n",
    "\n",
    "# Wordnet: Um repositório (tesauro) muito útil em PLN\n",
    "# Classificador Naive Bayes\n",
    "# Classificador Random forest"
   ]
  }
 ]
}