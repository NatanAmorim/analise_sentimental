{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIP - CC5P14 - Inteligencia Artificial (Prof: Aldriano Silva)\n",
    "* Esse script faz: Web Scraping, Análise sentimental, plotagem de gráficos.\n",
    "* Filtragem -> 'Stop Words' https://en.wikipedia.org/wiki/Stop_words\n",
    "* Modelo de armazenamento de dados -> 'Bag of Words' (Também conhecido como tokenização)\n",
    "* Testado em Python 3.7.6\n",
    "* Criado por:\n",
    "* Natan Amorim Souza Gomes de Moraes\n",
    "* Andrey Cardoso Gil\n",
    "\n",
    "* @TODO adicionar algoritimo de classificação 'Random Forest' ?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads\n",
    "\n",
    "## Beautiful Soup\n",
    "! pip install beautifulsoup4\n",
    "\n",
    "## Numpy\n",
    "! pip install numpy \n",
    "\n",
    "## Natural Language ToolKit (NLTK)\n",
    "! pip install nltk\n",
    "\n",
    "## SciKit Learn\n",
    "! pip install scipy\n",
    "! pip install scikit-learn\n",
    "\n",
    "## MatPlotLib\n",
    "! pip install matplotlib\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' +  \"Downloads feitos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "## Teste\n",
    "#import os\n",
    "import re\n",
    "\n",
    "## Teste 2\n",
    "#import sys\n",
    "#print(sys.argv[0])\n",
    "\n",
    "## JSON\n",
    "#import json\n",
    "\n",
    "## BeautifulSoup\n",
    "#from bs4 import BeautifulSoup \n",
    "\n",
    "## Numpy\n",
    "#import numpy as np \n",
    "\n",
    "## Natural Language ToolKit (NLTK)\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "## SciKit Learn\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## MatPlotLib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Importações feitas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw string\n",
    "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\"\n",
    "\n",
    "# Uma stopword pode ser considerada uma palavra irrelevante para a análise\n",
    "stopWords = ('a','ao','aos','aquela','aquelas','aquele','aqueles','aqui','aquilo','as','até','aí','com','como','da','das','de','dela', 'delas','dele','deles','depois','do','dos','dá','e','ela','elas','ele','eles','em','entre','era','eram','essa','essas','esse','esses','esta','estamos','estas','estava','estavam','este','esteja','estejam','estejamos','estes','esteve','estive', 'estivemos','estiver','estivera','estiveram','estiverem','estivermos','estivesse','estivessem','estivéramos','estivéssemos','estou','está','estávamos','estão','eu','foi','fomos','for','fora','foram','forem','formos','fosse','fossem','fui','fôramos','fôssemos','haja','hajam','hajamos','havemos','hei','houve','houvemos','houver','houvera','houveram','houverei','houverem','houveremos','houveria','houveriam','houvermos','houverá','houverão','houveríamos','houvesse','houvessem','houvéramos','houvéssemos','há','hão','isso','isto','já','lhe','lhes','lá','mais','mas','me','mesmo','meu','meus','minha','minhas','muito','na','nas','nem','no','nos','nossa','nossas','nosso','nossos','num','numa','não','nós','o','os','ou','para','pela','pelas','pelo','pelos','por','pra','qual','quando','que','quem','se','seja','sejam','sejamos','sem','serei','seremos','seria','seriam','será','serão','seríamos','seu','seus','somos','sou','sua','suas','são','só','também','te','tem','temos','tenha','tenham','tenhamos','tenho','terei','teremos','teria','teriam','terá','terão','teríamos','teu','teus','teve','tinha','tinham','tive','tivemos','tiver','tivera','tiveram','tiverem','tivermos','tivesse','tivessem','tivéramos','tivéssemos','tu','tua','tuas','tá','tém','tínhamos','um','uma','vai','você','vocês','vos','vou','à','às','é','éramos')\n",
    "\n",
    "frase = \"\"\"No meio do caminho tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\n",
    "Tinha uma pedra\n",
    "No meio do caminho tinha uma pedra\n",
    "\n",
    "Nunca me esquecerei desse acontecimento\n",
    "Na vida de minhas retinas tão fatigadas\n",
    "Nunca me esquecerei que no meio do caminho\n",
    "Tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\n",
    "No meio do caminho tinha uma pedra\"\"\"\n",
    "\n",
    "# identificação de palavras\n",
    "texto = re.findall(regex, frase)\n",
    "#for palavra in texto:\n",
    "    #print(palavra)\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Quantidade de palavras/tokens: {}\".format(len(texto)))\n",
    "\n",
    "# frequencia de a palavras no texto\n",
    "frequencia = dict([])\n",
    "for palavra in texto:\n",
    "    palavra = palavra.lower()\n",
    "    if palavra not in frequencia:\n",
    "        frequencia[palavra] = 0\n",
    "    frequencia[palavra] += 1\n",
    "print('\\033[1m' + '\\033[93m' + \"Palavras únicas: {}\\n\".format(len(frequencia)))\n",
    "\n",
    "# frequencia de a palavras no texto\n",
    "frequencia = dict([])\n",
    "for palavra in texto:\n",
    "    palavra = palavra.lower()\n",
    "    if palavra not in stopWords:\n",
    "        if palavra not in frequencia:\n",
    "            frequencia[palavra] = 0\n",
    "        frequencia[palavra] += 1\n",
    "print('\\033[1m' + '\\033[93m' + 'Palavras únicas sem \"Stop Words\": {}\\n'.format(len(frequencia)))\n",
    "\n",
    "# imprimir as 4 palavras mais frquentes\n",
    "maisFfrequentes = sorted(frequencia, key=frequencia.get, reverse=True)\n",
    "indexLista = 4\n",
    "for i in range(indexLista):\n",
    "    print('- \"{}\" apareceu {} vezes.'.format(maisFfrequentes[i], frequencia[maisFfrequentes[i]]))\n",
    "\n",
    "# Normalização de palavras\n",
    "# Lemmatization (a ação de reduzir em Lemmas)\n",
    "# Lemma: é a forma básica da palavra (sem inflexão)\n",
    "# Polissemia (muitos significados) é um problema\n",
    "# Sinônimos também podem dificultar a analise\n",
    "\n",
    "# Wordnet: Um repositório (tesauro) muito útil em PLN\n",
    "\n",
    "# sacoDePalavras segue o modelo de armazenamento de dados -> 'Bag of Words'\n",
    "# Bag-of-words: Conjunto não-ordenado de palavras (desconsidera a  gramática, mas mantendo a multiplicidade).\n",
    "sacoDePalavras = { \n",
    "    \"feliz\":5,\n",
    "    \"neutro\":2,\n",
    "    \"calmo\":0,\n",
    "    \"triste\":1,\n",
    "    \"raiva\":2,\n",
    "    \"desgosto\":0,\n",
    "    \"medo\":0,\n",
    "    \"surpreso\":2\n",
    "}\n",
    "\n",
    "print('\\n' + \"\\033[1m\" + \"\\033[92m\" + 'TESTE DE PLOT')\n",
    "plt.bar(list(bolsaDePalavras.keys()), bolsaDePalavras.values())\n",
    "plt.show()\n",
    "\n",
    "# Polissemia (muitos significados) é um problema\n",
    "# Sinônimos também podem dificultar a analise\n",
    "\n",
    "# Wordnet: Um repositório (tesauro) muito útil em PLN\n",
    "# Classificador Naive Bayes"
   ]
  }
 ]
}