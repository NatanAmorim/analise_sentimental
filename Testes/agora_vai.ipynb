{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: unidecode in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.1.1)\nRequirement already satisfied: pandas in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.0.3)\nRequirement already satisfied: pytz>=2017.2 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2019.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.1)\nRequirement already satisfied: numpy>=1.13.3 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.18.3)\nRequirement already satisfied: six>=1.5 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\nRequirement already satisfied: nltk in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.5)\nRequirement already satisfied: click in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (7.1.1)\nRequirement already satisfied: joblib in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (0.14.1)\nRequirement already satisfied: regex in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2020.4.4)\nRequirement already satisfied: tqdm in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.45.0)\nRequirement already satisfied: sklearn in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\nRequirement already satisfied: scikit-learn in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn) (0.22.2.post1)\nRequirement already satisfied: numpy>=1.11.0 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.18.3)\nRequirement already satisfied: scipy>=0.17.0 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\nRequirement already satisfied: seaborn in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.10.0)\nRequirement already satisfied: pandas>=0.22.0 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.0.3)\nRequirement already satisfied: numpy>=1.13.3 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.18.3)\nRequirement already satisfied: scipy>=1.0.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.4.1)\nRequirement already satisfied: matplotlib>=2.1.2 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2019.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\nRequirement already satisfied: six>=1.5 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.14.0)\nRequirement already satisfied: matplotlib in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.2.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\nRequirement already satisfied: numpy>=1.11 in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.18.3)\nRequirement already satisfied: six in c:\\users\\natan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n\n\u001b[1m\u001b[93mImporta√ß√µes feitas com sucesso!\n"
    }
   ],
   "source": [
    "#import string\n",
    "import re # Regex (express√£o regular)\n",
    "!pip install unidecode\n",
    "from unicodedata import normalize # ut√≠l para tratamento de texto e compatibilidade\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "!pip install nltk\n",
    "import nltk # Ferramentas de PLN\n",
    "!pip install sklearn\n",
    "import sklearn # Ferramentas de aprendizado de m√°quina\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt # Plotagem de gr√°ficosX_train_count\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Importa√ß√µes feitas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "* As seguintes CSV foram baixados em [Portuguese Tweets for Sentiment Analysis](https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis) criado por [augustop](https://www.kaggle.com/augustop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              tweet_text  sentiment\n0                @queren_renata Bom dia, √≥tima semana :)          1\n1                       o frio vai voltar finalmente :))          1\n2                     @kizqe damn fiquei a sorrir mto :)          1\n3      @lopes85 @_Goalpoint \"gentes\" do benfiquist√£o ...          1\n4      @crlhemely Exatamente! E a outra metade √© modi...          1\n...                                                  ...        ...\n39995              Vao cancelar minhas aulas de manh√£ :(          0\n39996  Triste a rejei√ß√£o do Haddad estar pr√≥xima a do...          0\n39997                  @vicevelyn_ Infelizmente tbm n :(          0\n39998                     @r9carloss No entendiii ptm :(          0\n39999                   S√≥ queria estar na minha cama :(          0\n\n[40000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@queren_renata Bom dia, √≥tima semana :)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>o frio vai voltar finalmente :))</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@kizqe damn fiquei a sorrir mto :)</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@lopes85 @_Goalpoint \"gentes\" do benfiquist√£o ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@crlhemely Exatamente! E a outra metade √© modi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>Vao cancelar minhas aulas de manh√£ :(</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>Triste a rejei√ß√£o do Haddad estar pr√≥xima a do...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>@vicevelyn_ Infelizmente tbm n :(</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>@r9carloss No entendiii ptm :(</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>S√≥ queria estar na minha cama :(</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows √ó 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis\n",
    "\n",
    "arquivo = 'Train50.csv'\n",
    "\n",
    "df_tweets = pd.read_csv(arquivo, sep=\";\", usecols=['tweet_text','sentiment'], index_col=None, header=0, dtype={\"sentiment\":\"int16\"})\n",
    "\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    20000\n0    20000\nName: sentiment, dtype: int64"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# √© importante que os dados estejam proximos para n√£o criar um 'Bias' muito grande a um tipo\n",
    "df_tweets['sentiment'].value_counts()\n",
    "# 1 = Positivo üôÇ\n",
    "# 0 = Negativo üôÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0        1\n1        1\n2        1\n3        1\n4        1\n        ..\n39995    0\n39996    0\n39997    0\n39998    0\n39999    0\nName: sentiment, Length: 40000, dtype: int16"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_tweets.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//# como o computador s√≥ entende n√∫meros n√≥s precisamos tranformar o maximo possivel em n√∫meros\n",
    "df_tweets['score'] = df_tweets['sentiment'].replace(['Negativo', 'Positivo'], [0, 1])\n",
    "\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                  @queren_renata bom dia, √≥tima semana :)\n1                         o frio vai voltar finalmente :))\n2                       @kizqe damn fiquei a sorrir mto :)\n3        @lopes85 @_goalpoint \"gentes\" do benfiquist√£o ...\n4        @crlhemely exatamente! e a outra metade √© modi...\n                               ...                        \n39995                vao cancelar minhas aulas de manh√£ :(\n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                    @vicevelyn_ infelizmente tbm n :(\n39998                       @r9carloss no entendiii ptm :(\n39999                     s√≥ queria estar na minha cama :(\nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Deixando tudo em min√∫sculo\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].str.lower()\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                  @queren_renata bom dia, √≥tima semana :)\n1                         o frio vai voltar finalmente :))\n2                       @kizqe damn fiquei a sorrir mto :)\n3        @lopes85 @_goalpoint \"gentes\" do benfiquist√£o ...\n4        @crlhemely exatamente! e a outra metade √© modi...\n                               ...                        \n39995                vao cancelar minhas aulas de manh√£ :(\n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                    @vicevelyn_ infelizmente tbm n :(\n39998                       @r9carloss no entendiii ptm :(\n39999                     s√≥ queria estar na minha cama :(\nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Removendo tudo entre []\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='\\[.*?\\]', value='', regex=True)\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover links\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='https?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='http?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='wwww?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='ftp?://[A-Za-z0-9./]+', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                  @queren_renata bom dia, √≥tima semana :)\n1                         o frio vai voltar finalmente :))\n2                       @kizqe damn fiquei a sorrir mto :)\n3        @lopes @_goalpoint \"gentes\" do benfiquist√£o n√£...\n4        @crlhemely exatamente! e a outra metade √© modi...\n                               ...                        \n39995                vao cancelar minhas aulas de manh√£ :(\n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                    @vicevelyn_ infelizmente tbm n :(\n39998                        @rcarloss no entendiii ptm :(\n39999                     s√≥ queria estar na minha cama :(\nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Removendo n√∫meros\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace='\\d+', value='', regex=True)\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                  @queren_renata bom dia, √≥tima semana :)\n1                         o frio vai voltar finalmente :))\n2                       @kizqe damn fiquei a sorrir mto :)\n3        @lopes @_goalpoint \"gentes\" do benfiquist√£o n√£...\n4        @crlhemely exatamente! e a outra metade √© modi...\n                               ...                        \n39995                vao cancelar minhas aulas de manh√£ :(\n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                    @vicevelyn_ infelizmente tbm n :(\n39998                        @rcarloss no entendiii ptm :(\n39999                     s√≥ queria estar na minha cama :(\nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Removendo #rashtags\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace=r'(\\#\\w+)', value='', regex=True)\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                 bom dia, √≥tima semana :)\n1                         o frio vai voltar finalmente :))\n2                              damn fiquei a sorrir mto :)\n3          \"gentes\" do benfiquist√£o n√£o gostam de futeb...\n4               exatamente! e a outra metade √© modinha :))\n                               ...                        \n39995                vao cancelar minhas aulas de manh√£ :(\n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                                infelizmente tbm n :(\n39998                                  no entendiii ptm :(\n39999                     s√≥ queria estar na minha cama :(\nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Removendo @Mentions\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace=r'(\\@\\w+)', value='', regex=True)\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                    bom dia √≥tima semana \n1                            o frio vai voltar finalmente \n2                                damn fiquei a sorrir mto \n3          gentes do benfiquist√£o n√£o gostam de futebol...\n4                   exatamente e a outra metade √© modinha \n                               ...                        \n39995                  vao cancelar minhas aulas de manh√£ \n39996    triste a rejei√ß√£o do haddad estar pr√≥xima a do...\n39997                                  infelizmente tbm n \n39998                                    no entendiii ptm \n39999                       s√≥ queria estar na minha cama \nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Removendo a pontua√ß√£o (EX: \"!?.,/|#$%¬®&\")\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                    bom dia otima semana \n1                            o frio vai voltar finalmente \n2                                damn fiquei a sorrir mto \n3          gentes do benfiquistao nao gostam de futebol...\n4                   exatamente e a outra metade e modinha \n                               ...                        \n39995                  vao cancelar minhas aulas de manha \n39996    triste a rejeicao do haddad estar proxima a do...\n39997                                  infelizmente tbm n \n39998                                    no entendiii ptm \n39999                       so queria estar na minha cama \nName: tweet_text, Length: 40000, dtype: object"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Removendo acentua√ß√£o\n",
    "df_tweets['tweet_text'] = df_tweets['tweet_text'].str.normalize('NFKD').str.encode('ASCII', errors='ignore').str.decode('UTF-8')\n",
    "\n",
    "df_tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\natan\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package rslp to\n[nltk_data]     C:\\Users\\natan\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Uma stopword pode ser considerada uma palavra irrelevante para a an√°lise\n",
    "nltk.download('stopwords')\n",
    "# RSLP(Removedor de Sufixos da L√≠ngua Portuguesa)\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa por tokens (tokens = palavras)\n",
    "#tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O processo de stemming consiste em reduzir a palavra √† sua raiz (sem levar em conta a classe gramatical)\n",
    "##stemmer = nltk.stem.RSLPStemmer() # RSLP(Removedor de Sufixos da L√≠ngua Portuguesa)\n",
    "##df_tweets['tweet_text_stemmed'] = df_tweets['tweet_text'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///# Nesse passo √© criado uma fun√ß√£o que faz a Tokeniza√ß√£o e o Stemming\n",
    "def criarBagOfWords(text, stop_words=stopWords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_tweets['bag_of_words'] = df_tweets['tweet_text'].apply(criarBagOfWords)\n",
    "\n",
    "df_tweets['bag_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9975a98ef2d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvetorizador\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvetorizador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Criando uma lista de stopWords\n",
    "stopWords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "\n",
    "vetorizador = CountVectorizer(stop_words=stopWords)\n",
    "#\n",
    "X = vetorizador.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vetorizador.fit_transform(df_tweets.tweet_text)\n",
    "y = df_tweets.sentiment\n",
    "\n",
    "# Separa em amostras de treino -> X_train, y_train e amostras de teste -> X_test y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "lower not found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d6308778f41e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvetorizador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#X_train_count = X_train_count.toarray()[:3]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1220\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#X_train_count = vetorizador.fit_transform(X_train.values)\n",
    "#X_train_count = X_train_count.toarray()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "modelo = naive_bayes.MultinomialNB()\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    'um bom dia',\n",
    "    'que coisa horrivel'\n",
    "]\n",
    "\n",
    "frases_count = vetorizador.transform(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 0], dtype=int16)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "modelo.predict(frases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7496"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "modelo.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit2a25272bf2b345eeaade93773a9e856a",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}