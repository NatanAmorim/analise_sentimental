{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import string\n",
    "import re # Regex (express√£o regular)\n",
    "!pip install unidecode\n",
    "from unicodedata import normalize # ut√≠l para tratamento de texto e compatibilidade\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "!pip install nltk\n",
    "import nltk # Ferramentas de PLN\n",
    "!pip install sklearn\n",
    "import sklearn\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt # Plotagem de gr√°ficos\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Importa√ß√µes feitas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "* As seguintes CSV foram baixados em [Portuguese Tweets for Sentiment Analysis](https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis) criado por [augustop](https://www.kaggle.com/augustop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentos = [\"Positivo\", \"Negativo\"]\n",
    "# PositivoüôÇ\n",
    "print(\"Positivo\" + \" \\U0001f642\")\n",
    "# Negativo üôÅ\n",
    "print(\"Negativo\" + \" \\U0001f641\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis\n",
    "\n",
    "arquivo = 'TweetsWithTheme.csv'\n",
    "\n",
    "df = pd.read_csv(arquivo, sep=\",\", index_col=None, header=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df[['tweet_text','sentiment']]\n",
    "\n",
    "# Limpando variaveis\n",
    "df = None\n",
    "del df\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √© importante que os dados estejam proximos para n√£o criar um 'Bias' muito grande a um tipo\n",
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = tweets['sentiment'].replace(['Negativo', 'Positivo'], [0, 1])\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'baseline' seria a performance de um classificador tirivial\n",
    "baseline = tweets['sentiment'].value_counts().max() / tweets['sentiment'].value_counts().sum()\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixando tudo em min√∫sculo\n",
    "tweets['tweet_text'] = tweets['tweet_text'].str.lower()\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo tudo entre []\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='\\[.*?\\]', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover links\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='https?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='http?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='wwww?://[A-Za-z0-9./]+', value='', regex=True)\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='ftp?://[A-Za-z0-9./]+', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo n√∫meros\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='\\d+', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo #rashtags\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'(\\#\\w+)', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo @Mentions\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'(\\@\\w+)', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a pontua√ß√£o (EX: \"!?.,/|#$%¬®&\")\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo acentua√ß√£o\n",
    "tweets['tweet_text'] = tweets['tweet_text'].str.normalize('NFKD').str.encode('ASCII', errors='ignore').str.decode('UTF-8')\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma stopword pode ser considerada uma palavra irrelevante para a an√°lise\n",
    "nltk.download('stopwords')\n",
    "# RSLP(Removedor de Sufixos da L√≠ngua Portuguesa)\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista de stopWords\n",
    "stopWords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "# O processo de stemming consiste em reduzir a palavra √† sua raiz (sem levar em conta a classe gramatical)\n",
    "stemmer = nltk.stem.RSLPStemmer() # RSLP(Removedor de Sufixos da L√≠ngua Portuguesa)\n",
    "# Separa por tokens (tokens = palavras)\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todasPalavras = ' '.join([text for text in tweets['tweet_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "tokensPalavras = tokenizer.tokenize(todasPalavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo StopWord e letras soltas do nosso texto\n",
    "todasPalavrasFiltradas = [palavra for palavra in tokensPalavras if palavra not in stopWords and len(palavra) > 2]\n",
    "\n",
    "#todasPalavrasFiltradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesse passo √© criado uma fun√ß√£o que faz a Tokeniza√ß√£o e o Stemming\n",
    "def criarBagOfWords(text, stop_words=stopWords):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "tweets['bag_of_words'] = tweets['tweet_text'].apply(criarBagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['bag_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o\n",
    "def tokenizador(texto):\n",
    "    return re.findall(r\"[-'a-zA-Z√Ä-√ñ√ò-√∂√∏-√ø0]+\", texto)\n",
    "\n",
    "tweets['bag_of_words'] = tweets['tweet_text'].apply(tokenizador)\n",
    "\n",
    "tweets['bag_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar as 10 palavras mais frequentes\n",
    "\n",
    "plotar = False # Precisa ser True para plotar\n",
    "\n",
    "if(plotar):\n",
    "    todasPalavras = ' '.join([text for text in tweets['bag_of_words']])\n",
    "    tokensPalavras = tokenizer.tokenize(todasPalavras)\n",
    "    frequenciaPalavras = nltk.FreqDist(todasPalavrasFiltradas)\n",
    "    df_frequencia_palavras = pd.DataFrame({\n",
    "        'palavra': list(frequenciaPalavras.keys()),\n",
    "        'frequencia': list(frequenciaPalavras.values())\n",
    "    })\n",
    "    maisFrequentes = df_frequencia_palavras.nlargest(columns='frequencia', n=10)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data=maisFrequentes, x='palavra', y='frequencia')\n",
    "    ax.set(ylabel='frequencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tail()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}