{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import string\n",
    "import re # Regex (expressão regular)\n",
    "!pip install unidecode\n",
    "from unicodedata import normalize # utíl para tratamento de texto e compatibilidade\n",
    "!pip install nltk\n",
    "import nltk # Ferramentas de PLN\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt # Plotagem de gráficos\n",
    "\n",
    "print(\"\\n\" + '\\033[1m' + '\\033[93m' + \"Importações feitas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "* As seguintes CSV foram baixados em [Portuguese Tweets for Sentiment Analysis](https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis) criado por [augustop](https://www.kaggle.com/augustop) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/augustop/portuguese-tweets-for-sentiment-analysis\n",
    "arquivos = [\n",
    "    'TweetsNeutralHash.csv',\n",
    "    'TweetsNeutralNews.csv',\n",
    "    'TweetsWithTheme.csv'\n",
    "]\n",
    "\n",
    "lista = list()\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    df = pd.read_csv(arquivo, sep=\",\", index_col=None, header=0)\n",
    "    lista.append(df[['tweet_text','sentiment']])\n",
    "\n",
    "tweets = pd.concat(lista, axis=0, ignore_index=True)\n",
    "\n",
    "tweets['score'] = tweets['sentiment'].replace(['Negativo','Neutro', 'Positivo'], [-1, 0, 1])\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpando variaveis\n",
    "arquivo = None\n",
    "arquivos = None\n",
    "df = None\n",
    "lista = None\n",
    "\n",
    "del arquivo\n",
    "del arquivos\n",
    "del df\n",
    "del lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixando tudo em minúsculo\n",
    "tweets['tweet_text'] = tweets['tweet_text'].str.lower()\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo tudo entre []\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='\\[.*?\\]', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo números\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace='\\d+', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo #rashtags\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'(\\#\\w+)', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo @Mentions\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'(\\@\\w+)', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a pontuação (EX: \"!?.,/|#$%¨&\")\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo acentuação\n",
    "tweets['tweet_text'] = tweets['tweet_text'].str.normalize('NFKD').str.encode('ASCII', errors='ignore').str.decode('UTF-8')\n",
    "\n",
    "tweets['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma stopword pode ser considerada uma palavra irrelevante para a análise\n",
    "nltk.download('stopwords')\n",
    "# RSLP(Removedor de Sufixos da Língua Portuguesa)\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista de stopWords\n",
    "stopWords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "# O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical)\n",
    "stemmer = nltk.stem.RSLPStemmer() # RSLP(Removedor de Sufixos da Língua Portuguesa)\n",
    "# Separa por tokens (tokens = palavras)\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "todasPalavras = ' '.join([text for text in tweets['tweet_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensPalavras = tokenizer(todasPalavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo StopWord do nosso texto\n",
    "todasPalavrasFiltradas = [palavra for palavra in tokensPalavras if palavra not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequenciaPalavras = nltk.FreqDist(todasPalavrasFiltradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencia_palavras = pd.DataFrame({\n",
    "    'palavra': list(frequenciaPalavras.keys()),\n",
    "    'frequencia': list(frequenciaPalavras.values())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maisFrequentes = df_frequencia_palavras.nlargest(columns='frequencia', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.barplot(data=maisFrequentes, x='palavra', y='frequencia')\n",
    "ax.set(ylabel='frequencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesse passo é criado uma função que faz a Tokenização e o Stemming\n",
    "def criarBagOfWords(texto, stop_words=stopWords):\n",
    "    teste = list()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print('-' * 30)\n",
    "    print(tokens)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    print('-' * 30)\n",
    "    print(tokens)\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    print('-' * 30)\n",
    "    print(tokens)\n",
    "    #return ' '.join(tokens)\n",
    "    #return \n",
    "\n",
    "# Bag-of-words: Conjunto não-ordenado de palavras e sua multiplicidade\n",
    "# Cria um \"Bag of Words (Saco de Palavras)\" do nosso texto e ignora números\n",
    "\n",
    "#tweets['bag_of_words'] = tweets['tweet_text'].apply(criarBagOfWords)\n",
    "#tweets['bag_of_words_stemmed'] = tweets['tweet_text'].apply(criarBagOfWords)\n"
   ]
  }
 ]
}